image: docker:stable

variables:
   # When using dind service we need to instruct docker, to talk with the
   # daemon started inside of the service. The daemon is available with
   # a network connection instead of the default /var/run/docker.sock socket.
   #
   # The 'docker' hostname is the alias of the service container as described at
   # https://docs.gitlab.com/ee/ci/docker/using_docker_images.html#accessing-the-services
   #
   # Note that if you're using the Kubernetes executor, the variable should be set to
   # tcp://localhost:2375/ because of how the Kubernetes executor connects services
   # to the job container
   # DOCKER_HOST: tcp://localhost:2375/
   #
   # For non-Kubernetes executors, we use tcp://docker:2375/
   DOCKER_HOST: tcp://docker:2375/
   # When using dind, it's wise to use the overlayfs driver for
   # improved performance.
   DOCKER_DRIVER: overlay2
   
   # Since the docker:dind container and the runner container don’t share their root
   # filesystem, the job’s working directory can be used as a mount point for children
   # containers. For example, if you have files you want to share with a child container,
   # you may create a subdirectory under /builds/$CI_PROJECT_PATH and use it as your
   # mount point.
   MOUNT_POINT: /builds/$CI_PROJECT_PATH/mnt
   
   # For EBI you need to override the definition of CI_REGISTRY to remove the port number
   CI_REGISTRY: dockerhub.ebi.ac.uk
   CI_REGISTRY_IMAGE: $CI_REGISTRY/$CI_PROJECT_PATH

   #NOW: $(date '+%Y-%m-%d-%H-%M-%S')
   #NOW: $(date '+%Y-%m-%d')
   
   # To solve the issue with the Docker in Docker 19.03 service.
   # Logged as: GitLab.com CI jobs failing if using docker:stable-dind image
   # see: https://gitlab.com/gitlab-com/gl-infra/production/issues/982
   DOCKER_TLS_CERTDIR: ""

services:
   - docker:dind


stages:
   - build
   - sandbox-deploy
   - dev-deploy



build_image:
    stage: build
    except:
        - schedules
        - triggers
        - pipelines
    script:
        - docker info
        - mkdir -p "$MOUNT_POINT"
        - echo "${CI_REGISTRY_PASSWORD}" | docker login -u "${CI_REGISTRY_USER}" --password-stdin  ${CI_REGISTRY}
   
        - cd impc_prod_tracker
    
         
        # Try to pull the image from the registry so that it can be used as a cache for the docker build command
        - docker pull $CI_REGISTRY_IMAGE:latest || true
        
        - docker build --cache-from $CI_REGISTRY_IMAGE:latest -t "${CI_REGISTRY_IMAGE}":"${CI_COMMIT_SHA:0:12}" -t "${CI_REGISTRY_IMAGE}":latest .  | tee ${MOUNT_POINT}/build.log
                
        - docker push "${CI_REGISTRY_IMAGE}"  | tee ${MOUNT_POINT}/push.log
       
        - docker logout ${CI_REGISTRY}

        - |
          if [[ "${DOCKER_HUB_PUSH}" == "true" ]]; then
              
              echo "${DOCKER_HUB_PSWD}" | docker login -u "${DOCKER_HUB_USER}" --password-stdin
              docker tag "${CI_REGISTRY_IMAGE}":"${CI_COMMIT_SHA:0:12}" "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}":"${CI_COMMIT_SHA:0:12}"
              docker tag "${CI_REGISTRY_IMAGE}":"${CI_COMMIT_SHA:0:12}" "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}":latest
              docker push "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}"  | tee ${MOUNT_POINT}/dockerhub-push-latest.log
              docker logout
          fi    
    artifacts:
        paths:
            - "$MOUNT_POINT/"


# build_jars:
#   stage: build
#   image: maven:3.6.3-jdk-11
#   script:
#     - 'mvn -f impc_prod_tracker/pom.xml -s .gitlab-ci_settings.xml clean deploy'
#   only:
#     - master
#   except:
#     - schedules
#     - triggers
#     - pipelines
  


sandbox:
  stage: sandbox-deploy
  image: dtzar/helm-kubectl:2.13.0
  only:
    refs:
      - master
    variables:
      - $KUBERNETES_DEPLOYMENT == "true"
  script:
  # Only deploy from the MPI2 impc-production-tracker repository rather than repository forks
  - |
    if [ ! -z ${KUBERNETES_ENDPOINT+set} ]; then

      kubectl config set-cluster local --server="${KUBERNETES_ENDPOINT}"
      kubectl config set clusters.local.certificate-authority-data "${KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
      kubectl config set-credentials "${KUBERNETES_USER}" --token="${KUBERNETES_USER_TOKEN}"
      kubectl config set-context "${KUBERNETES_NAMESPACE}" --cluster=local --user=${KUBERNETES_USER} --namespace="${KUBERNETES_NAMESPACE}"
      kubectl config use-context "${KUBERNETES_NAMESPACE}"
      kubectl version
     
      cd impc_prod_tracker
     
      #
      #
      # Substitute the "latest" image tag in your deployment template with a more specific tag
      # and record the deployment so you can rollback to this particular version.
      #
      sed -i "s/latest/${SANDBOX_API_TAG}/g" kube/sandbox/api-service/api-service-deployment.yaml
      sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/sandbox/api-service/api-service-deployment.yaml

      if kubectl apply -f kube/sandbox/api-service/api-service-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/sandbox/api-service/api-service-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi
      #
      #
      # Log the status of the application deployment
      #
      kubectl rollout status -f kube/sandbox/api-service/api-service-deployment.yaml
      kubectl get all,ing
    fi
  


hh-sandbox:
  stage: sandbox-deploy
  image: dtzar/helm-kubectl:2.14.3
  only:
    refs:
      - master
    variables:
      - $KUBERNETES_DEPLOYMENT == "true"
  script:
  # Only deploy from the MPI2 impc-production-tracker repository rather than repository forks
  - |
    if [ ! -z ${HH_KUBERNETES_ENDPOINT+set} ]; then

      kubectl config set-cluster local --server="${HH_KUBERNETES_ENDPOINT}"
      kubectl config set clusters.local.certificate-authority-data "${HH_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
      kubectl config set-credentials ${HH_KUBERNETES_SANDBOX_USER} --token="${HH_KUBERNETES_SANDBOX_USER_TOKEN}"
      kubectl config set-context "${HH_KUBERNETES_SANDBOX_NAMESPACE}" --cluster=local --user=${HH_KUBERNETES_SANDBOX_USER} --namespace="${HH_KUBERNETES_SANDBOX_NAMESPACE}"
      kubectl config use-context "${HH_KUBERNETES_SANDBOX_NAMESPACE}"
      kubectl version
     
      cd impc_prod_tracker
     
      #
      #
      # Substitute the "latest" image tag in your deployment template with a more specific tag
      # and record the deployment so you can rollback to this particular version.
      #
      sed -i "s/latest/${SANDBOX_API_TAG}/g" kube/wp/sandbox/api-service/api-service-deployment.yaml
      sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/wp/sandbox/api-service/api-service-deployment.yaml

      if kubectl apply -f kube/wp/sandbox/api-service/api-service-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/wp/sandbox/api-service/api-service-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi
      #
      #
      # Log the status of the application deployment
      #
      kubectl rollout status -f kube/wp/sandbox/api-service/api-service-deployment.yaml
      kubectl get pod,deployment,rs,svc,ing
    fi
  


hx-sandbox:
  stage: sandbox-deploy
  image: dtzar/helm-kubectl:2.14.3
  only:
    refs:
      - master
    variables:
      - $KUBERNETES_DEPLOYMENT == "true"
  script:
  # Only deploy from the MPI2 impc-production-tracker repository rather than repository forks
  - |
    if [ ! -z ${HX_KUBERNETES_ENDPOINT+set} ]; then

      kubectl config set-cluster local --server="${HX_KUBERNETES_ENDPOINT}"
      kubectl config set clusters.local.certificate-authority-data "${HX_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
      kubectl config set-credentials ${HX_KUBERNETES_SANDBOX_USER} --token="${HX_KUBERNETES_SANDBOX_USER_TOKEN}"
      kubectl config set-context "${HX_KUBERNETES_SANDBOX_NAMESPACE}" --cluster=local --user=${HX_KUBERNETES_SANDBOX_USER} --namespace="${HX_KUBERNETES_SANDBOX_NAMESPACE}"
      kubectl config use-context "${HX_KUBERNETES_SANDBOX_NAMESPACE}"
      kubectl version
     
      cd impc_prod_tracker
     
      #
      #
      # Substitute the "latest" image tag in your deployment template with a more specific tag
      # and record the deployment so you can rollback to this particular version.
      #
      sed -i "s/latest/${SANDBOX_API_TAG}/g" kube/wp/sandbox/api-service/api-service-deployment.yaml
      sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/wp/sandbox/api-service/api-service-deployment.yaml

      if kubectl apply -f kube/wp/sandbox/api-service/api-service-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/wp/sandbox/api-service/api-service-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi
      #
      #
      # Log the status of the application deployment
      #
      kubectl rollout status -f kube/wp/sandbox/api-service/api-service-deployment.yaml
      kubectl get pod,deployment,rs,svc,ing
    fi




dev:
  stage: dev-deploy
  image: dtzar/helm-kubectl:2.13.0
  only:
    refs:
      - master
    variables:
      - $KUBERNETES_DEPLOYMENT == "true"
  script:
  # Only deploy from the MPI2 impc-production-tracker repository rather than repository forks
  - |
    if [ ! -z ${KUBERNETES_ENDPOINT+set} ]; then
    
      apk --no-cache --update add jq curl
      
      kubectl config set-cluster local --server="${KUBERNETES_ENDPOINT}"
      kubectl config set clusters.local.certificate-authority-data "${KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
      kubectl config set-credentials "${KUBERNETES_DEV_USER}" --token="${KUBERNETES_DEV_USER_TOKEN}"
      kubectl config set-context "${KUBERNETES_DEV_NAMESPACE}" --cluster=local --user=${KUBERNETES_DEV_USER} --namespace="${KUBERNETES_DEV_NAMESPACE}"
      kubectl config use-context "${KUBERNETES_DEV_NAMESPACE}"
      kubectl version
     
      cd impc_prod_tracker
     
      #
      #
      # Substitute the "latest" image tag in your deployment template with a more specific tag
      # and record the deployment so you can rollback to this particular version.
      #
      
      # Need to check that the docker image corresponding to this CI_COMMIT_SHA exists in Docker Hub.
      # Scheduled turnover of the container can still run for a CI_COMMIT_SHA where the build fail to complete.
      
      TOKEN=$( curl -sSLd "username=${DOCKER_HUB_USER}&password=${DOCKER_HUB_PSWD}" https://hub.docker.com/v2/users/login | jq -r ".token" )
      JSON=$( curl -sH "Authorization: JWT $TOKEN" "https://hub.docker.com/v2/repositories/${DOCKER_HUB_USER}/${DOCKER_HUB_REPO}/tags/${CI_COMMIT_SHA:0:12}/")
      IMAGE_TAG_NAME=$(echo $JSON | jq  -r '.name')
      
      if [ "$IMAGE_TAG_NAME" = "${CI_COMMIT_SHA:0:12}" ]; then
          # Substitute the "latest" image tag in your deployment template
          sed -i "s/latest/${CI_COMMIT_SHA:0:12}/g" kube/dev/api-service/api-service-deployment.yaml
      else
          # DO NOT push out the latest tag to ensure rollback is possible.
          exit 1
      fi
      
      
      sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/dev/api-service/api-service-deployment.yaml

      if kubectl apply -f kube/dev/api-service/api-service-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/dev/api-service/api-service-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi
      #
      #
      # Log the status of the application deployment
      #
      kubectl rollout status -f kube/dev/api-service/api-service-deployment.yaml
      kubectl get all,ing
    fi




hh-dev:
  stage: dev-deploy
  image: dtzar/helm-kubectl:2.13.0
  only:
    refs:
      - master
    variables:
      - $KUBERNETES_DEPLOYMENT == "true"
  script:
  # Only deploy from the MPI2 impc-production-tracker repository rather than repository forks
  - |
    if [ ! -z ${HH_KUBERNETES_ENDPOINT+set} ]; then
    
      apk --no-cache --update add jq curl

      kubectl config set-cluster local --server="${HH_KUBERNETES_ENDPOINT}"
      kubectl config set clusters.local.certificate-authority-data "${HH_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
      kubectl config set-credentials ${HH_KUBERNETES_DEV_USER} --token="${HH_KUBERNETES_DEV_USER_TOKEN}"
      kubectl config set-context "${HH_KUBERNETES_DEV_NAMESPACE}" --cluster=local --user=${HH_KUBERNETES_DEV_USER} --namespace="${HH_KUBERNETES_DEV_NAMESPACE}"
      kubectl config use-context "${HH_KUBERNETES_DEV_NAMESPACE}"
      kubectl version
     
      cd impc_prod_tracker
     
      #
      #
      # Substitute the "latest" image tag in your deployment template with a more specific tag
      # and record the deployment so you can rollback to this particular version.
      #
      
      # Need to check that the docker image corresponding to this CI_COMMIT_SHA exists in Docker Hub.
      # Scheduled turnover of the container can still run for a CI_COMMIT_SHA where the build fail to complete.
      
      TOKEN=$( curl -sSLd "username=${DOCKER_HUB_USER}&password=${DOCKER_HUB_PSWD}" https://hub.docker.com/v2/users/login | jq -r ".token" )
      JSON=$( curl -sH "Authorization: JWT $TOKEN" "https://hub.docker.com/v2/repositories/${DOCKER_HUB_USER}/${DOCKER_HUB_REPO}/tags/${CI_COMMIT_SHA:0:12}/")
      IMAGE_TAG_NAME=$(echo $JSON | jq  -r '.name')
      
      if [ "$IMAGE_TAG_NAME" = "${CI_COMMIT_SHA:0:12}" ]; then
          # Substitute the "latest" image tag in your deployment template
          sed -i "s/latest/${CI_COMMIT_SHA:0:12}/g" kube/wp/dev/api-service/api-service-deployment.yaml
      else
          # DO NOT push out the latest tag to ensure rollback is possible.
          exit 1
      fi
      
      
      sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/wp/dev/api-service/api-service-deployment.yaml

      if kubectl apply -f kube/wp/dev/api-service/api-service-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/wp/dev/api-service/api-service-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi
      #
      #
      # Log the status of the application deployment
      #
      kubectl rollout status -f kube/wp/dev/api-service/api-service-deployment.yaml
      kubectl get pod,deployment,rs,svc,ing
    fi




hx-dev:
  stage: dev-deploy
  image: dtzar/helm-kubectl:2.13.0
  only:
    refs:
      - master
    variables:
      - $KUBERNETES_DEPLOYMENT == "true"
  script:
  # Only deploy from the MPI2 impc-production-tracker repository rather than repository forks
  - |
    if [ ! -z ${HX_KUBERNETES_ENDPOINT+set} ]; then
    
      apk --no-cache --update add jq curl

      kubectl config set-cluster local --server="${HX_KUBERNETES_ENDPOINT}"
      kubectl config set clusters.local.certificate-authority-data "${HX_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
      kubectl config set-credentials ${HX_KUBERNETES_DEV_USER} --token="${HX_KUBERNETES_DEV_USER_TOKEN}"
      kubectl config set-context "${HX_KUBERNETES_DEV_NAMESPACE}" --cluster=local --user=${HX_KUBERNETES_DEV_USER} --namespace="${HX_KUBERNETES_DEV_NAMESPACE}"
      kubectl config use-context "${HX_KUBERNETES_DEV_NAMESPACE}"
      kubectl version
     
      cd impc_prod_tracker
     
      #
      #
      # Substitute the "latest" image tag in your deployment template with a more specific tag
      # and record the deployment so you can rollback to this particular version.
      #
      
      # Need to check that the docker image corresponding to this CI_COMMIT_SHA exists in Docker Hub.
      # Scheduled turnover of the container can still run for a CI_COMMIT_SHA where the build fail to complete.
      
      TOKEN=$( curl -sSLd "username=${DOCKER_HUB_USER}&password=${DOCKER_HUB_PSWD}" https://hub.docker.com/v2/users/login | jq -r ".token" )
      JSON=$( curl -sH "Authorization: JWT $TOKEN" "https://hub.docker.com/v2/repositories/${DOCKER_HUB_USER}/${DOCKER_HUB_REPO}/tags/${CI_COMMIT_SHA:0:12}/")
      IMAGE_TAG_NAME=$(echo $JSON | jq  -r '.name')
      
      if [ "$IMAGE_TAG_NAME" = "${CI_COMMIT_SHA:0:12}" ]; then
          # Substitute the "latest" image tag in your deployment template
          sed -i "s/latest/${CI_COMMIT_SHA:0:12}/g" kube/wp/dev/api-service/api-service-deployment.yaml
      else
          # DO NOT push out the latest tag to ensure rollback is possible.
          exit 1
      fi
      
      
      sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/wp/dev/api-service/api-service-deployment.yaml

      if kubectl apply -f kube/wp/dev/api-service/api-service-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/wp/dev/api-service/api-service-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi
      #
      #
      # Log the status of the application deployment
      #
      kubectl rollout status -f kube/wp/dev/api-service/api-service-deployment.yaml
      kubectl get pod,deployment,rs,svc,ing
    fi
